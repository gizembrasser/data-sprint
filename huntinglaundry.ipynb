{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeGJvfgmI8Vq"
      },
      "source": [
        "# Exploring the Economics of Russian State Media Content Laundering\n",
        "\n",
        "Recent research shows that networks of websites, which serve as pathways for Russian state and pro-Kremlin media, are able to operate in EU member countries despite clear restrictions in place.\n",
        "\n",
        "This is mostly achieved through networks of mirror websites that by posing as ‘independent’ and / or ‘alternative’ news outlets are able to distribute by stealth Russian state media content. They operate either under new domain names or domain names that are related to the banned outlets. One such example is sputnikglobe.com - a mirror site of Sputnik News that is up and running in countries like the UK and the Netherlands.\n",
        "\n",
        "\n",
        "This dataset includes mirror websites/reposted Op-eds for banned Russian newssite RT.com, resulting from running the domain through disinfo.id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "ZhoLIcjBI5HP",
        "outputId": "60abcd95-54b3-4447-c587-660f6f6a6af4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5afcbd78-69b6-43ae-989c-2fa200fba646\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5afcbd78-69b6-43ae-989c-2fa200fba646\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Output - Op-eds Russia Today (1).xlsx to Output - Op-eds Russia Today (1) (2).xlsx\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRdQ0-sZFJUm"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "Load the Excel file into a pandas DataFrame. Some records are tagged as social media posts or other irrelevant sites (such as RT.com itself). We will filter those out for now. We'll also adjust the value for the Domain name to include 'https://' to be able to use them with the `requests`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odFzgYVaJ4-o"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "content = pd.read_excel(\"Output - Op-eds Russia Today (1).xlsx\")\n",
        "content = content[content[\"Irrelevant sites\"].isnull()]\n",
        "content[\"Domain\"] = \"https://\" + content[\"Domain\"]\n",
        "content.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spFDw1aOOC_C"
      },
      "source": [
        "### Global Variables\n",
        "\n",
        "We want to compare the list of resulting sites that could be possible mirrors for RT. We also need to provide a User-Agent for the request to be able to scrape the data from certain sites."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_A_rOB1aTRVF"
      },
      "outputs": [],
      "source": [
        "SEARCHED_URL = \"https://www.rt.com\"\n",
        "RESULT_DOMAINS = content[\"Domain\"].unique().tolist()\n",
        "USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_QC5D__brc8"
      },
      "source": [
        "## Metadata analysis\n",
        "\n",
        "According to disinfo.id there are certain metadata indicators that make RT.com unique. The Verification ID is a unique identifier used for verifying ownership of authenticity of a website or online account. They can establish the legitimacy of a site or account, potentially linking it to a specific owner or organization.\n",
        "\n",
        "We will use a function to scrape metatags from the source website (RT.com) and extract any whose name attribute contains the word 'verification'. Save the value for the content attribute for those in a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eODZuywfM1SU"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup, SoupStrainer\n",
        "\n",
        "def fetch_verification_tags(url, timeout=10):\n",
        "  \"\"\"\n",
        "  Fetch meta tags whose names contain 'verification' from the given URL.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    # Set a User-Agent and timeout for the HTTP request\n",
        "    headers = {\"User-Agent\": USER_AGENT}\n",
        "    response = requests.get(url, headers=headers, timeout=timeout)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Parse only the meta tags to improve performance\n",
        "    parse_only = SoupStrainer(\"meta\")\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\", parse_only=parse_only)\n",
        "\n",
        "    # Find all meta tags with 'name' attribute containing 'verification'\n",
        "    verification_tags = soup.find_all(\"meta\", attrs={\"name\": lambda x: x and \"verification\" in x.lower()})\n",
        "    # Extract 'content' attribute\n",
        "    if verification_tags:\n",
        "      return {tag['name']: tag.get('content', '') for tag in verification_tags if tag.has_attr('content')}\n",
        "    else:\n",
        "      return {}\n",
        "  except Exception as e:\n",
        "    print(f\"Error fetching verification tags from {url}: {e}\")\n",
        "    return {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v6kjC7GUMoD"
      },
      "source": [
        "Next we'll use a function that gathers the unique result URLs from the DataFrame, and compares the tags to the ones for RT.com. If one or more of the tags have matching Verification IDs, it's probably a mirror site. If this is the case, adjust the column 'Mirror / Reposter?' for that URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjHWrJM3V4or"
      },
      "outputs": [],
      "source": [
        "def compare_tags(source_url, target_urls, df):\n",
        "    \"\"\"\n",
        "    Compare verification meta tags between a source URL and a list of target URLs.\n",
        "    \"\"\"\n",
        "    source_tag = fetch_verification_tags(source_url)\n",
        "\n",
        "    for target_url in target_urls:\n",
        "      target_tag = fetch_verification_tags(target_url)\n",
        "\n",
        "      # Check for matches with each dictionary in target_tags list\n",
        "      matches = {name: content for name, content in target_tag.items() if name in source_tag and source_tag[name] == content}\n",
        "\n",
        "      if matches:\n",
        "        for name, content in matches.items():\n",
        "          print(f\"Matching metatags: {target_url}\")\n",
        "        # Update the 'Mirror / Reposter?' column\n",
        "        df.loc[df[\"Domain\"] == target_url, \"Mirror / Reposter?\"] = \"Mirror\"\n",
        "      else:\n",
        "        df.loc[df[\"Domain\"] == target_url, \"Mirror / Reposter?\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds837IudPm1Z"
      },
      "source": [
        "## CSS class analysis\n",
        "\n",
        "Other metadata identifiers are the CSS classes for the websites. Similairly designed websites will share CSS classes. We will scrape all the unique CSS classes from RT.com."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOO8NiJRQNgq"
      },
      "outputs": [],
      "source": [
        "def scrape_css(url, timeout=10):\n",
        "  try:\n",
        "    headers = {\"User-Agent\": USER_AGENT}\n",
        "    response = requests.get(url, headers=headers, timeout=timeout)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Parse only the meta tags to improve performance\n",
        "    parse_only = SoupStrainer(\"meta\")\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\", parse_only=parse_only)\n",
        "\n",
        "    used_classes = set()\n",
        "\n",
        "    for elem in soup.select(\"[class]\"):\n",
        "      classes = elem[\"class\"]\n",
        "      used_classes.update(classes)\n",
        "\n",
        "    return used_classes\n",
        "  except Exception as e:\n",
        "    print(f\"Error scraping CSS classes from {url}: {e}\")\n",
        "    return set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFEJC33KTdgM"
      },
      "source": [
        "Compare the unique CSS classes from the source URL to the ones from the dataset. If 90% of the classes match, return the URL for the mirror website."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDK5Xpu1Thm7"
      },
      "outputs": [],
      "source": [
        "def compare_css(source_url, target_urls):\n",
        "    \"\"\"\n",
        "    Compare CSS classes from a base URL to CSS classes from a list of other URLs.\n",
        "    Return URLs with 90% or more common classes.\n",
        "    \"\"\"\n",
        "    source_css_classes = scrape_css(source_url)\n",
        "\n",
        "    matching_urls = []\n",
        "    for url in target_urls:\n",
        "        css_classes = scrape_css(url)\n",
        "        common_classes = source_css_classes.intersection(css_classes)\n",
        "\n",
        "        # Calculate the percentage of common classes\n",
        "        percentage_common = len(common_classes) / len(source_css_classes)\n",
        "\n",
        "        if percentage_common >= 0.9:\n",
        "          matching_urls.append(url)\n",
        "\n",
        "    return matching_urls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztTDLGOjQMW5"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "First we will compare the Verification IDs. Print the matching tags and update the DataFrame when a site is classified as a mirror to RT.com. Some sites may still result in errors, so check those manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5wNSzFxhWcdA"
      },
      "outputs": [],
      "source": [
        "rt_tags = fetch_verification_tags(SEARCHED_URL)\n",
        "matching_tags = compare_tags(SEARCHED_URL, RESULT_DOMAINS, content)\n",
        "print(matching_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMOPStP_cQER"
      },
      "source": [
        "Compare CSS classes from all the unique domains to RT.com. Are the results the same as the matches that came up when comparing the Verification IDs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "o0-9iwEecGp3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f83290a-4d12-46d1-b0b5-97ed1b8aecef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error scraping CSS classes from https://www.taghribnews.com: HTTPSConnectionPool(host='www.taghribnews.com', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x79d7f66777f0>, 'Connection to www.taghribnews.com timed out. (connect timeout=10)'))\n",
            "Error scraping CSS classes from https://pk.shafaqna.com: 500 Server Error: Internal Server Error for url: https://pk.shafaqna.com/\n",
            "Error scraping CSS classes from https://www.pk.shafaqna.com: 500 Server Error: Internal Server Error for url: https://www.pk.shafaqna.com/\n",
            "Error scraping CSS classes from https://archive.ph: HTTPSConnectionPool(host='archive.ph', port=443): Read timed out. (read timeout=10)\n",
            "Error scraping CSS classes from https://www.lebanonnewsapp.com: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "Error scraping CSS classes from https://news.nestia.com: 404 Client Error: Not Found for url: https://news.nestia.com/\n",
            "Error scraping CSS classes from https://detv.us: Exceeded 30 redirects.\n",
            "Error scraping CSS classes from https://exceptionalinsights.group: HTTPSConnectionPool(host='exceptionalinsights.group', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)')))\n",
            "Error scraping CSS classes from https://www.theinteldrop.org: 403 Client Error: Forbidden for url: https://www.theinteldrop.org/\n",
            "Error scraping CSS classes from https://www.infowars.com: 403 Client Error: Forbidden for url: https://www.infowars.com/\n",
            "Error scraping CSS classes from https://www.qatarnewsapp.com: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "Error scraping CSS classes from https://qatarnewsapp.com: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "Error scraping CSS classes from https://intlecity.com: HTTPSConnectionPool(host='intlecity.com', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x79d7f8352980>, 'Connection to intlecity.com timed out. (connect timeout=10)'))\n",
            "Error scraping CSS classes from https://wawaforum.com: HTTPSConnectionPool(host='wawaforum.com', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x79d7f70e3340>, 'Connection to wawaforum.com timed out. (connect timeout=10)'))\n",
            "Error scraping CSS classes from https://lebanonnewsapp.com: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "Error scraping CSS classes from https://protonmail24435.lt.acemlnb.com: 403 Client Error: Forbidden for url: https://protonmail24435.lt.acemlnb.com/\n",
            "Error scraping CSS classes from https://www.geopolitic.ro: HTTPSConnectionPool(host='www.geopolitic.ro', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\n",
            "Error scraping CSS classes from https://boards.4chan.org: 403 Client Error: Forbidden for url: https://www.4chan.org/\n",
            "Error scraping CSS classes from https://192.16.75.145: HTTPSConnectionPool(host='192.16.75.145', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: IP address mismatch, certificate is not valid for '192.16.75.145'. (_ssl.c:1007)\")))\n",
            "Error scraping CSS classes from https://uaenewsapp.com: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "Error scraping CSS classes from https://dehai.org: HTTPSConnectionPool(host='dehai.org', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate (_ssl.c:1007)')))\n",
            "Error scraping CSS classes from https://5f4d928bd.netherlandsvsecuadorlive.one: HTTPSConnectionPool(host='5f4d928bd.netherlandsvsecuadorlive.one', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1007)')))\n",
            "Error scraping CSS classes from https://www.tarikcyrilamar.com: 404 Client Error: Not Found for url: https://www.tarikcyrilamar.com/\n",
            "Error scraping CSS classes from https://www.flashback.org: HTTPSConnectionPool(host='www.flashback.org', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x79d7f8f27430>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Error scraping CSS classes from https://worldnewspapes.com: HTTPSConnectionPool(host='worldnewspapes.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate (_ssl.c:1007)')))\n",
            "Error scraping CSS classes from https://d14fe3e70.giadinhvang.com: HTTPSConnectionPool(host='d14fe3e70.giadinhvang.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1007)')))\n",
            "Error scraping CSS classes from https://www.moonofalabama.org: 403 Client Error: Forbidden for url: https://www.moonofalabama.org/\n",
            "Error scraping CSS classes from https://183.89.207.204:853: HTTPSConnectionPool(host='183.89.207.204', port=853): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1007)')))\n",
            "Error scraping CSS classes from https://vucanews.com: HTTPSConnectionPool(host='vucanews.com', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x79d7f6a5bca0>, 'Connection to vucanews.com timed out. (connect timeout=10)'))\n",
            "Error scraping CSS classes from https://www.unz.com: 403 Client Error: Forbidden for url: https://www.unz.com/\n",
            "Error scraping CSS classes from https://www.sotwe.com: 403 Client Error: Forbidden for url: https://www.sotwe.com/\n",
            "Error scraping CSS classes from https://45.63.30.15: HTTPSConnectionPool(host='45.63.30.15', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x79d7f89a2860>, 'Connection to 45.63.30.15 timed out. (connect timeout=10)'))\n",
            "Error scraping CSS classes from https://www.dailydumper.com: HTTPSConnectionPool(host='www.dailydumper.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'www.dailydumper.com'. (_ssl.c:1007)\")))\n",
            "Error scraping CSS classes from https://inventsolitude.sblo.jp: HTTPSConnectionPool(host='inventsolitude.sblo.jp', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'inventsolitude.sblo.jp'. (_ssl.c:1007)\")))\n",
            "Error scraping CSS classes from https://www.zbcnews.co.zw: HTTPSConnectionPool(host='www.zbcnews.co.zw', port=443): Read timed out. (read timeout=10)\n",
            "Error scraping CSS classes from https://neweralive.na: HTTPSConnectionPool(host='neweralive.na', port=443): Read timed out. (read timeout=10)\n",
            "Error scraping CSS classes from https://www.freeintertv.com: HTTPSConnectionPool(host='www.freeintertv.com', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x79d7f70e1c60>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "Error scraping CSS classes from https://tv.ethiovisit.com: HTTPSConnectionPool(host='tv.ethiovisit.com', port=443): Read timed out. (read timeout=10)\n",
            "Error scraping CSS classes from https://stats.diasp.eu: HTTPSConnectionPool(host='stats.diasp.eu', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLCertVerificationError(1, \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'stats.diasp.eu'. (_ssl.c:1007)\")))\n",
            "['https://swentr.site', 'https://admin.sonsuz1.com', 'https://doc.swentr.site']\n"
          ]
        }
      ],
      "source": [
        "rt_classes = scrape_css(SEARCHED_URL)\n",
        "matching_css = compare_css(SEARCHED_URL, RESULT_DOMAINS)\n",
        "print(matching_css)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8dzgsUQcSYu"
      },
      "source": [
        "Which sites are exact mirrors to RT.com, and what location are they based in?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "fcb4J0QRcH4P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10ede223-bbca-4006-903a-46e91a334f77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Domain: https://swentr.site\n",
            "Locations: ['USA', 'USA', 'USA', 'USA']\n",
            "Domain: https://admin.sonsuz1.com\n",
            "Locations: ['USA']\n",
            "Domain: https://doc.swentr.site\n",
            "Locations: ['USA']\n"
          ]
        }
      ],
      "source": [
        "filtered_content = content[content[\"Mirror / Reposter?\"] == \"Mirror\"] # Filter rows with non null values\n",
        "unique_mirrors = filtered_content[\"Domain\"].unique().tolist() # Extract those values of the 'Domain' column\n",
        "\n",
        "for domain in unique_mirrors:\n",
        "  rows = filtered_content[filtered_content[\"Domain\"] == domain]\n",
        "  locations = rows[\"Location\"].tolist()\n",
        "  print(f\"Domain: {domain}\")\n",
        "  print(f\"Locations: {locations}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp80Km6MOcec"
      },
      "source": [
        "Save the resulting DataFrame to an Excel file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rzL8DbgO8qo"
      },
      "outputs": [],
      "source": [
        "content.to_excel(\"Output - Op-eds Russia Today (1).xlsx\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}